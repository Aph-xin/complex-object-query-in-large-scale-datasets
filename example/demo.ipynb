{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "import PIL.Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from encoder.owl_reranker import (OwlPredictor, OwlDecodeOutput)\n",
    "from encoder.owl_drawing import draw_owl_output\n",
    "\n",
    "# torch.cuda.set_device(0)\n",
    "\n",
    "logger = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_dir, log_filename):\n",
    "    # Set the global logger variable\n",
    "    global logger  \n",
    "    # Create the log directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the log file path\n",
    "    log_file = os.path.join(log_dir, log_filename)\n",
    "    # Set the log format\n",
    "    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    \n",
    "    # Set up the logging configuration\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=log_format,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),    # Log to file\n",
    "            logging.StreamHandler()                  # Log to console\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get the logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    # Log the initialization message\n",
    "    logger.info(f\"Logging initialized. Logs are being saved to {log_file}\")\n",
    "    # Return the logger\n",
    "    return logger\n",
    "\n",
    "\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        start_time = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        logger.info(f\"{method.__name__} executed in {elapsed_time:.2f} seconds\")\n",
    "        return result\n",
    "    return timed\n",
    "\n",
    "\n",
    "# Connect to Milvus and create collection\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "    if utility.has_collection(collection_name):\n",
    "        logger.info(f\"Dropping existing collection {collection_name}\")\n",
    "        utility.drop_collection(collection_name)\n",
    "\n",
    "    fields = [\n",
    "        FieldSchema(\n",
    "            name=\"id\", \n",
    "            dtype=DataType.INT64, \n",
    "            is_primary=True, \n",
    "            auto_id=True\n",
    "        ),\n",
    "        FieldSchema(\n",
    "            name=\"image_id\",\n",
    "            dtype=DataType.VARCHAR, \n",
    "            max_length=255\n",
    "        ),\n",
    "        FieldSchema(\n",
    "            name=\"image_embeds\", \n",
    "            dtype=DataType.FLOAT_VECTOR, \n",
    "            dim=dim\n",
    "        ),\n",
    "        FieldSchema(\n",
    "            name=\"pred_boxes\", \n",
    "            dtype=DataType.FLOAT_VECTOR, \n",
    "            dim=4\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    schema = CollectionSchema(\n",
    "        fields=fields, \n",
    "        description=\"Image embeddings with predicted boxes\"\n",
    "    )\n",
    "    collection = Collection(\n",
    "        name=collection_name, \n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    index_params = {\n",
    "        \"metric_type\": \"COSINE\", \n",
    "        \"index_type\": \"IVF_FLAT\", \n",
    "        \"params\": {\"nlist\": 512}\n",
    "    }\n",
    "    collection.create_index(\n",
    "        field_name=\"image_embeds\", \n",
    "        index_params=index_params\n",
    "    )\n",
    "    collection.create_index(\n",
    "        field_name=\"pred_boxes\", \n",
    "        index_params=index_params\n",
    "    )\n",
    "    collection.load()\n",
    "\n",
    "    logger.info(f\"Collection {collection_name} created successfully with embedding dimension {dim}\")\n",
    "    return collection\n",
    "\n",
    "def load_milvus_collection(collection_name):\n",
    "    connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "    if utility.has_collection(collection_name):\n",
    "        logger.info(f\"Collection {collection_name} already exists. Loading the collection.\")\n",
    "        collection = Collection(collection_name)\n",
    "        collection.load()  # Load the existing collection into memory\n",
    "        logger.info(f\"Collection {collection_name} loaded successfully.\")\n",
    "\n",
    "        return collection\n",
    "\n",
    "# Encode images and insert into Milvus\n",
    "def encode_and_store_images(predictor, image_path, collection):\n",
    "    image = load_image(image_path)\n",
    "    output = predictor.image_encoder_milvus(image=image, pad_square=False)\n",
    "\n",
    "    image_embeds = output.image_class_embeds_aug.squeeze().cpu().detach().numpy()\n",
    "    pred_boxes = output.pred_boxes.squeeze().cpu().detach().numpy()\n",
    "\n",
    "    data = [{\"image_embeds\": patch_embed.tolist(), \"pred_boxes\": pred_box.tolist(), \"image_id\": image_path}\n",
    "            for patch_embed, pred_box in zip(image_embeds, pred_boxes)]\n",
    "    \n",
    "    try:\n",
    "        collection.insert(data)\n",
    "        # logger.info(f\"Image {image_path} encoded and stored successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error inserting to Milvus: {e}\")\n",
    "\n",
    "\n",
    "# Load and ensure images are in RGB format\n",
    "def load_image(image_path):\n",
    "    image = PIL.Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "@timeit\n",
    "def upload_frame(predictor, collection, df, interval):\n",
    "    for index in tqdm(range(0, df.shape[0], interval), total=(df.shape[0] // interval), desc=\"Inserting frames\"):\n",
    "            image_path = df.iloc[index]['path']\n",
    "            encode_and_store_images(predictor, image_path, collection)\n",
    "    logger.info(f\"All the {df.shape[0]} frames uploaded in {collection.name} successfully\")\n",
    "\n",
    "\n",
    "# Encode text prompts\n",
    "def encode_query(predictor, prompt):\n",
    "    prompt = prompt.strip(\"][()\")\n",
    "    texts = prompt.split(',')\n",
    "    text_embed = predictor.encode_text([texts])\n",
    "    return text_embed\n",
    "\n",
    "\n",
    "# Search for similar images in Milvus using text embeddings\n",
    "@timeit\n",
    "def search_similar_images(text_output, collection, top_n):\n",
    "    text_embeds = text_output.text_embeds.squeeze().cpu().detach().numpy()\n",
    "    text_embeds = text_embeds[np.newaxis, :] if len(text_embeds.shape) == 1 else text_embeds\n",
    "\n",
    "    search_params = {\n",
    "        \"metric_type\": \"COSINE\", \n",
    "        \"params\": {\"nprobe\": 10}\n",
    "    }\n",
    "    results = [collection.search([\n",
    "        embed.astype(float)], \n",
    "        anns_field=\"image_embeds\", \n",
    "        param=search_params, \n",
    "        limit=top_n,\n",
    "        output_fields=[\"pred_boxes\", \"image_id\"]\n",
    "    ) for embed in text_embeds]\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groundingdino.datasets.transforms as T\n",
    "from groundingdino.models import build_model\n",
    "from groundingdino.util import box_ops\n",
    "from groundingdino.util.slconfig import SLConfig\n",
    "from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap\n",
    "from groundingdino.util.vl_utils import create_positive_map_from_span\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def get_grounding_output_whole(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False):\n",
    "    \"\"\"\n",
    "    Generate detection boxes and scores for a full-sentence caption without splitting it.\n",
    "    \"\"\"\n",
    "    caption = caption.lower().strip()\n",
    "    if not caption.endswith(\".\"):\n",
    "        caption += \".\"\n",
    "        \n",
    "    device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image[None], captions=[caption])\n",
    "    \n",
    "    # Extract confidence scores and detection boxes\n",
    "    logits = outputs[\"pred_logits\"].sigmoid()[0]  # (nq, 256)\n",
    "    boxes = outputs[\"pred_boxes\"][0]  # (nq, 4)\n",
    "\n",
    "    # Calculate the highest relevance score for each box with the full caption\n",
    "    scores = logits.max(dim=1)[0]  # Get the highest score for each detection box\n",
    "    filt_mask = scores > box_threshold  # Filter low-score boxes using box_threshold\n",
    "    boxes_filt = boxes[filt_mask]\n",
    "    scores_filt = scores[filt_mask].tolist()  # Convert filtered scores to a list\n",
    "\n",
    "    # If with_logits is specified, use the full caption as labels with scores\n",
    "    if with_logits:\n",
    "        pred_phrases = [f\"{caption} ({score:.2f})\" for score in scores_filt]\n",
    "    else:\n",
    "        pred_phrases = [caption for _ in scores_filt]\n",
    "\n",
    "    return boxes_filt, pred_phrases, scores_filt\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load an image and preprocess it for model input.\n",
    "    \"\"\"\n",
    "    image_pil = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image, _ = transform(image_pil, None)\n",
    "    return image_pil, image\n",
    "\n",
    "def load_model(model_config_path, model_checkpoint_path, cpu_only=False):\n",
    "    \"\"\"\n",
    "    Load the Grounding DINO model from config and checkpoint files.\n",
    "    \"\"\"\n",
    "    args = SLConfig.fromfile(model_config_path)\n",
    "    args.device = \"cuda\" if not cpu_only else \"cpu\"\n",
    "    model = build_model(args)\n",
    "    checkpoint = torch.load(model_checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n",
    "    return model.eval()\n",
    "\n",
    "def dino_rerank(config_file, checkpoint_path, image_path, text_prompt, box_threshold=0.3, text_threshold=0.1, cpu_only=False):\n",
    "    \"\"\"\n",
    "    Perform reranking using Grounding DINO and return results in a compatible format.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image_pil, image = load_image(image_path)\n",
    "\n",
    "    # Load the model\n",
    "    model = load_model(config_file, checkpoint_path, cpu_only=cpu_only)\n",
    "\n",
    "    # Run the model to get detection boxes and scores\n",
    "    boxes_filt, pred_phrases, scores = get_grounding_output_whole(\n",
    "        model, image, text_prompt, box_threshold, text_threshold, cpu_only=cpu_only\n",
    "    )\n",
    "\n",
    "    # Construct pred_dict in a format compatible with reranking\n",
    "    output = {\n",
    "        \"boxes\": boxes_filt,\n",
    "        \"labels\": pred_phrases,\n",
    "        \"scores\": scores,\n",
    "    }\n",
    "\n",
    "    # Extract the highest score for reranking judgment\n",
    "    max_score = max(scores) if scores else 0.0\n",
    "\n",
    "    return output, max_score\n",
    "\n",
    "def save_unique_dino_rerank_log(rerank_results, save_dir, top_k):\n",
    "    \"\"\"\n",
    "    Sort rerank results by score in descending order and save to a log file.\n",
    "    Only keep unique image_ids, selecting the highest score for each.\n",
    "    Save the top_k unique image_ids and their scores.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Sort by score in descending order and deduplicate\n",
    "    unique_results = {}\n",
    "    for image_id, score, output_rerank in rerank_results:\n",
    "        if image_id not in unique_results or score > unique_results[image_id][0]:\n",
    "            unique_results[image_id] = (score, output_rerank)\n",
    "\n",
    "    # Prepare sorted unique results, limited to top_k\n",
    "    sorted_unique_results = sorted(unique_results.items(), key=lambda x: x[1][0], reverse=True)[:top_k]\n",
    "\n",
    "    # Prepare log entries\n",
    "    log_entries = [f\"Image ID: {image_id}, Score: {score}\" for image_id, (score, _) in sorted_unique_results]\n",
    "\n",
    "    # Write to log file\n",
    "    log_file_path = os.path.join(save_dir, \"top_k_rerank_log.txt\")\n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        for entry in log_entries:\n",
    "            log_file.write(entry + \"\\n\")\n",
    "    \n",
    "    logger.info(f\"Top {top_k} unique re-rank results log saved to {log_file_path}\")\n",
    "\n",
    "    return sorted_unique_results\n",
    "\n",
    "@timeit\n",
    "def unique_dino_reranker(config_file, checkpoint_path, results, prompts, box_threshold, text_threshold, top_k, save_dir):\n",
    "    \"\"\"\n",
    "    Perform unique reranking using DINO and save the top_k results.\n",
    "    \"\"\"\n",
    "    rerank_results = []\n",
    "\n",
    "    # Start recording rerank time\n",
    "    rerank_start_time = time.time()\n",
    "\n",
    "    for result_group in results:\n",
    "        for result in result_group:\n",
    "            for match in result:\n",
    "                image_id = match.entity.get(\"image_id\")\n",
    "                output_rerank, max_score = dino_rerank(config_file, checkpoint_path, image_id, prompts, box_threshold, text_threshold)\n",
    "                \n",
    "                rerank_results.append((image_id, max_score, output_rerank))\n",
    "\n",
    "    # End rerank timing and log the duration\n",
    "    rerank_time = time.time() - rerank_start_time\n",
    "    logger.info(f\"Re-rank processing time: {rerank_time:.2f} seconds\")\n",
    "    \n",
    "    # Save the log, ensuring each image_id keeps only the highest score and recording the top_k results\n",
    "    final_result = save_unique_dino_rerank_log(rerank_results, save_dir, top_k)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxes_to_image(image_pil, tgt):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on the provided PIL image based on target data.\n",
    "    \"\"\"\n",
    "    H, W = tgt[\"size\"]\n",
    "    boxes = tgt[\"boxes\"]\n",
    "    labels = tgt[\"labels\"]\n",
    "    assert len(boxes) == len(labels), \"boxes and labels must have the same length\"\n",
    "\n",
    "    # Get the device\n",
    "    device = boxes.device  # Retrieve the device where boxes are located\n",
    "    draw = ImageDraw.Draw(image_pil)\n",
    "    mask = Image.new(\"L\", image_pil.size, 0)\n",
    "    mask_draw = ImageDraw.Draw(mask)\n",
    "\n",
    "    # Convert box size info to match the device of boxes\n",
    "    size_tensor = torch.Tensor([W, H, W, H]).to(device)  # Ensure device consistency\n",
    "\n",
    "    # Draw boxes and masks\n",
    "    for box, label in zip(boxes, labels):\n",
    "        # Convert from normalized (0..1) to image dimensions (0..W, 0..H)\n",
    "        box = box * size_tensor\n",
    "        # Convert from xywh format to xyxy format\n",
    "        box[:2] -= box[2:] / 2\n",
    "        box[2:] += box[:2]\n",
    "        # Random color\n",
    "        color = tuple(np.random.randint(0, 255, size=3).tolist())\n",
    "        # Draw\n",
    "        x0, y0, x1, y1 = box\n",
    "        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=color, width=6)\n",
    "        # draw.text((x0, y0), str(label), fill=color)\n",
    "\n",
    "        font = ImageFont.load_default()\n",
    "        if hasattr(font, \"getbbox\"):\n",
    "            bbox = draw.textbbox((x0, y0), str(label), font)\n",
    "        else:\n",
    "            w, h = draw.textsize(str(label), font)\n",
    "            bbox = (x0, y0, w + x0, y0 + h)\n",
    "        draw.rectangle(bbox, fill=color)\n",
    "        draw.text((x0, y0), str(label), fill=\"white\")\n",
    "\n",
    "        mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)\n",
    "\n",
    "    return image_pil, mask\n",
    "\n",
    "\n",
    "def save_final_image(result, save_dir):\n",
    "    \"\"\"\n",
    "    Extract boxes, labels, and scores from unique_dino_rerank results, draw them on the image, and save it.\n",
    "    :param result: A single result from unique_dino_rerank, containing boxes, labels, and scores\n",
    "    :param save_dir: Directory to save the resulting images\n",
    "    \"\"\"\n",
    "    # Save the final images\n",
    "    for image_path, output_rerank in result:\n",
    "        # Retrieve detection results\n",
    "        boxes_filt = output_rerank[1]['boxes']\n",
    "        labels = output_rerank[1]['labels']\n",
    "        scores = output_rerank[1]['scores']\n",
    "\n",
    "        # Ensure the save directory exists\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Load the original image\n",
    "        image_pil, _ = load_image(image_path)  # Assumes each result includes an image_path\n",
    "        size = image_pil.size\n",
    "\n",
    "        # Draw detection boxes on the image\n",
    "        pred_dict = {\n",
    "            \"boxes\": boxes_filt,\n",
    "            \"size\": [size[1], size[0]],  # Height, width\n",
    "            \"labels\": labels,\n",
    "            \"scores\": scores\n",
    "        }\n",
    "\n",
    "        # Use plot_boxes_to_image to draw boxes\n",
    "        image_with_boxes, _ = plot_boxes_to_image(image_pil, pred_dict)\n",
    "\n",
    "        # Get the filename without the full path\n",
    "        image_filename = os.path.basename(image_path)  # Keep only the filename, excluding the path\n",
    "\n",
    "        # Optional: Replace invalid characters in the filename (e.g., spaces)\n",
    "        image_filename = image_filename.replace(\" \", \"_\")  # Replace spaces with underscores\n",
    "\n",
    "        # Create the save path\n",
    "        image_save_path = os.path.join(save_dir, f\"{scores}_{image_filename}_result.png\")\n",
    "\n",
    "        # Ensure the save directory exists (redundant but kept for clarity)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Save the image\n",
    "        image_with_boxes.save(image_save_path)\n",
    "\n",
    "        print(f\"Saved image with boxes to {image_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import PIL.Image\n",
    "from encoder.owl_drawing import draw_owl_output\n",
    "from encoder.owl_reranker import OwlDecodeOutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Save image with the filename format \"image_id_scores_model_name_threshold.png\"\n",
    "def save_image_with_boxes(image_with_boxes, data_name, image_id, max_score, model_name, threshold, save_dir):\n",
    "    \"\"\"\n",
    "    Save an image with bounding boxes to the specified directory with a formatted filename.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    base_image_id = os.path.splitext(os.path.basename(image_id))[0]\n",
    "\n",
    "    file_name = f\"{data_name}_{max_score:.2f}_{model_name}_thresh{threshold}_{base_image_id}.jpg\"\n",
    "    file_path = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    image_with_boxes.save(file_path)\n",
    "    logger.info(f\"Frame saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Display search results with bounding boxes\n",
    "def display_results_from_milvus(results):\n",
    "    \"\"\"\n",
    "    Display search results from Milvus with bounding boxes drawn on the images.\n",
    "    \"\"\"\n",
    "    for result_group in results:\n",
    "        for result in result_group:\n",
    "            for match in result:\n",
    "                image_id = match.entity.get(\"image_id\")\n",
    "                pred_boxes = match.entity.get(\"pred_boxes\")\n",
    "                pred_boxes = [float(x) for x in pred_boxes]\n",
    "                texts = [\"Object\"] * len(pred_boxes)\n",
    "\n",
    "                boxes_tensor = torch.tensor(pred_boxes).reshape(1, 4)\n",
    "                labels_tensor = torch.zeros((1), dtype=torch.int64)\n",
    "\n",
    "                output = OwlDecodeOutput(\n",
    "                    labels=labels_tensor, scores=torch.ones_like(labels_tensor),\n",
    "                    boxes=boxes_tensor, input_indices=torch.zeros_like(labels_tensor)\n",
    "                )\n",
    "                \n",
    "                image = PIL.Image.open(image_id)\n",
    "                image_with_boxes = draw_owl_output(image, output, texts)\n",
    "                \n",
    "                plt.figure(figsize=(5, 5))\n",
    "                plt.imshow(image_with_boxes)\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "def save_rerank_log(rerank_results, save_dir):\n",
    "    \"\"\"\n",
    "    Sort rerank results by score in descending order and save them to a log file.\n",
    "    \"\"\"\n",
    "    # Ensure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Sort by score in descending order\n",
    "    rerank_results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Prepare log entries\n",
    "    log_entries = [f\"Image ID: {image_id}, Score: {score}\" for image_id, score, _ in rerank_results]\n",
    "\n",
    "    # Write to log file\n",
    "    log_file_path = os.path.join(save_dir, \"rerank_log.txt\")\n",
    "    with open(log_file_path, \"w\") as log_file:\n",
    "        for entry in log_entries:\n",
    "            log_file.write(entry + \"\\n\")\n",
    "    \n",
    "    logger.info(f\"Re-rank results log saved to {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 01:03:57,332 - __main__ - INFO - Logging initialized. Logs are being saved to ../logs/cityscapes/0217_cityscapes_q11.log\n",
      "/home/aph/anaconda3/envs/dino/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "2025-02-19 01:04:01,548 - __main__ - INFO - Collection cityscapes_vit32_new already exists. Loading the collection.\n",
      "2025-02-19 01:04:01,561 - __main__ - INFO - Collection cityscapes_vit32_new loaded successfully.\n",
      "2025-02-19 01:04:01,562 - __main__ - INFO - Using prompt: [a person wearing the black suit walking on the crosswalk]\n",
      "2025-02-19 01:04:01,562 - __main__ - INFO - Using rerank model: Vit-B-32\n",
      "2025-02-19 01:04:01,562 - __main__ - INFO - Using output path: ../results/0217_10_cityscapes_q11\n",
      "2025-02-19 01:04:02,299 - __main__ - INFO - search_similar_images executed in 0.20 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aph/anaconda3/envs/dino/lib/python3.10/site-packages/transformers/modeling_utils.py:1161: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/aph/anaconda3/envs/dino/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-19 01:08:09,654 - __main__ - INFO - Re-rank processing time: 247.35 seconds\n",
      "2025-02-19 01:08:09,657 - __main__ - INFO - Top 10 unique re-rank results log saved to ../results/0217_10_cityscapes_q11/top_k_rerank_log.txt\n",
      "2025-02-19 01:08:09,658 - __main__ - INFO - unique_dino_reranker executed in 247.36 seconds\n",
      "2025-02-19 01:08:09,658 - __main__ - INFO - Total execution time: 252.33 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # --------------------- Logger Setup ---------------------\n",
    "    logger = setup_logging(\n",
    "        log_dir=\"../logs/cityscapes\",\n",
    "        log_filename=\"cityscapes_query.log\"\n",
    "    )\n",
    "\n",
    "    # --------------------- Timing Setup ---------------------\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --------------------- Dataset Preparation ---------------------\n",
    "    database_name = \"cityscapes_vit32_new\"\n",
    "    dataset = \"cityscapes\"\n",
    "    df = pd.read_csv('../dataset/cityscapes/stuggart_overall.csv')\n",
    "    top_n = 10\n",
    "    top_k = 10\n",
    "    threshold = 0.1\n",
    "    interval = 1\n",
    "\n",
    "    # --------------------- Model Initialization ---------------------\n",
    "    predictor = OwlPredictor(\n",
    "        model_name=\"google/owlvit-base-patch32\",\n",
    "        image_encoder_engine=None\n",
    "    )\n",
    "\n",
    "    # --------------------- Database Preparation ---------------------\n",
    "    # Create and populate Milvus collection\n",
    "    collection = create_milvus_collection(database_name, 512)\n",
    "    upload_frame(predictor, collection, df, interval)\n",
    "    collection = load_milvus_collection(database_name)\n",
    "\n",
    "    # --------------------- Query Setup ---------------------\n",
    "    prompts = \"[a person wearing the black suit walking on the crosswalk]\"\n",
    "    model_prefix = \"Vit-B-32\"\n",
    "    save_dir = \"../results/cityscapes_query\"\n",
    "\n",
    "    logger.info(f\"Using prompt: {prompts}\")\n",
    "    logger.info(f\"Using rerank model: {model_prefix}\")\n",
    "    logger.info(f\"Using output path: {save_dir}\")\n",
    "\n",
    "    # --------------------- Fast Search ---------------------\n",
    "    text_output = encode_query(predictor, prompts)\n",
    "    results = search_similar_images(text_output, collection, top_n)\n",
    "\n",
    "    # --------------------- DINO Rerank Setup ---------------------\n",
    "    config_file = \"./GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "    checkpoint_path = \"./GroundingDINO/weight/groundingdino_swint_ogc.pth\"\n",
    "    text_prompt = \"a person wearing the black suit walking on the crosswalk\"\n",
    "    box_threshold, text_threshold = 0.3, 0.1\n",
    "\n",
    "    # --------------------- Rerank and Saving ---------------------\n",
    "    final_result = unique_dino_reranker(\n",
    "        config_file=config_file,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        results=results,\n",
    "        prompts=text_prompt,\n",
    "        box_threshold=box_threshold,\n",
    "        text_threshold=text_threshold,\n",
    "        top_k=top_k,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "\n",
    "    # --------------------- Timing Summary ---------------------\n",
    "    total_time = time.time() - start_time\n",
    "    logger.info(f\"Total execution time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8568341732025146, 0.33085355162620544]_stuttgart_01_000000_004472_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8560197353363037, 0.3609195351600647]_stuttgart_02_000000_005189_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8553482294082642, 0.36383795738220215]_stuttgart_02_000000_005190_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8549236059188843, 0.3472067415714264]_stuttgart_02_000000_005196_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8536893725395203, 0.348927766084671, 0.3035251200199127]_stuttgart_02_000000_005197_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8509528636932373, 0.32841941714286804]_stuttgart_01_000000_004473_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8506414294242859, 0.31499800086021423]_stuttgart_02_000000_005613_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8502881526947021, 0.3264321982860565]_stuttgart_01_000000_004471_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8497912287712097]_stuttgart_02_000000_005612_leftImg8bit.png_result.png\n",
      "Saved image with boxes to ../results/0217_10_cityscapes_q11/[0.8483469486236572, 0.34902164340019226, 0.33062833547592163]_stuttgart_02_000000_005194_leftImg8bit.png_result.png\n"
     ]
    }
   ],
   "source": [
    "save_final_image(final_result, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
